from datetime import datetime
import re
import aiofiles
import logging
import collections
from collections import defaultdict
from nltk.util import ngrams
from aiogram import types
from config import LOG_FILE
from prompts import STOPWORDS

# –ó–∞–ø–∏—Å—å —Å–æ–æ–±—â–µ–Ω–∏–π –≤—Å–µ—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –≤ —Ñ–∞–π–ª
async def save_user_message(message: types.Message):
    timestamp = datetime.now().isoformat()
    chat_id = message.chat.id if message.chat else "NoChat"
    chat_title = message.chat.title if message.chat and message.chat.title else "–õ–°"
    user_id = message.from_user.id
    username = message.from_user.username or "NoUsername"
    full_name = message.from_user.full_name or "NoName"
    text = message.text or ""
    log_line = f"{timestamp} - Chat {chat_id} ({chat_title}) - User {user_id} ({username}) [{full_name}]: {text}\n"

    try:
        async with aiofiles.open(LOG_FILE, mode="a", encoding="utf-8") as f:
            await f.write(log_line)
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –≤ {LOG_FILE}: {e}")

# üìå –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π –ø–æ ID
async def extract_user_messages(user_id: int, chat_id: int) -> list:
    messages = []
    pattern = re.compile(rf".* - Chat {chat_id}\b.*User {user_id}\b.*: (.*)")
    async with aiofiles.open(LOG_FILE, mode="r", encoding="utf-8") as f:
        async for line in f:
            match = pattern.match(line)
            if match:
                messages.append(match.group(1).strip())
    return messages

async def extract_messages_by_username(username: str, chat_id: int) -> list:
    messages = []
    pattern = re.compile(rf".* - Chat {chat_id}\b.*User \d+ \(({re.escape(username)})\) \[.*?\]: (.*)")
    async with aiofiles.open(LOG_FILE, mode="r", encoding="utf-8") as f:
        async for line in f:
            match = pattern.match(line)
            if match:
                messages.append(match.group(2).strip())
    return messages

async def extract_messages_by_full_name(full_name: str, chat_id: int) -> list:
    messages = []
    pattern = re.compile(rf".* - Chat {chat_id}\b.*User \d+ \([^)]+\) \[(.+?)\]: (.*)")
    async with aiofiles.open(LOG_FILE, mode="r", encoding="utf-8") as f:
        async for line in f:
            match = pattern.match(line)
            if match and match.group(1).lower() == full_name.lower():
                messages.append(match.group(2).strip())
    return messages

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π –≤—Å–µ–≥–æ —á–∞—Ç–∞ –ø–æ chat_id
async def extract_chat_messages(chat_id: int) -> list:
    messages = []
    pattern = re.compile(rf".* - Chat {chat_id}\b - User .+?: (.*)")
    async with aiofiles.open(LOG_FILE, mode="r", encoding="utf-8") as f:
        async for line in f:
            match = pattern.match(line)
            if match:
                messages.append(match.group(1).strip())
    return messages

# üìå –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ (—É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤)
def clean_text(text: str) -> list:
    words = re.findall(r"\w+", text.lower())
    filtered_words = [word for word in words if word not in STOPWORDS]
    return filtered_words

# üìå –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤
async def get_frequent_words(user_id: int, top_n: int = 10):
    messages = await extract_user_messages(user_id)
    all_text = " ".join(messages)
    words = clean_text(all_text)
    counter = collections.Counter(words)
    return counter.most_common(top_n)

# üìå –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —á–∞—Å—Ç–æ —É–ø–æ—Ç—Ä–µ–±–ª—è–µ–º—ã—Ö —Ñ—Ä–∞–∑
async def get_frequent_phrases(user_id: int, n: int = 2, top_n: int = 10):
    messages = await extract_user_messages(user_id)
    all_text = " ".join(messages)
    words = clean_text(all_text)
    if len(words) < n:
        return []
    ngram_list = list(ngrams(words, n))
    ngram_counter = collections.Counter(ngram_list)
    return [(" ".join(gram), count) for gram, count in ngram_counter.most_common(top_n)]

# üìå –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ñ—Ä–∞–∑ –ø–æ —Ç–µ–∫—Å—Ç—É (–ø–æ username/full_name)
def get_frequent_phrases_from_text(text: str, n: int = 2, top_n: int = 5) -> list:
    words = clean_text(text)
    if len(words) < n:
        return []
    ngram_list = list(ngrams(words, n))
    ngram_counter = collections.Counter(ngram_list)
    return [(" ".join(gram), count) for gram, count in ngram_counter.most_common(top_n)]

# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤ –∏ —Ñ—Ä–∞–∑ –¥–ª—è —á–∞—Ç–∞
async def get_chat_frequent_words(chat_id: int, top_n: int = 10):
    messages = await extract_chat_messages(chat_id)
    all_text = " ".join(messages)
    words = clean_text(all_text)
    counter = collections.Counter(words)
    return counter.most_common(top_n)

async def get_chat_frequent_phrases(chat_id: int, n: int = 2, top_n: int = 10):
    messages = await extract_chat_messages(chat_id)
    all_text = " ".join(messages)
    words = clean_text(all_text)
    if len(words) < n:
        return []
    ngram_list = list(ngrams(words, n))
    ngram_counter = collections.Counter(ngram_list)
    return [(" ".join(gram), count) for gram, count in ngram_counter.most_common(top_n)]

# üÜï –ù–û–í–ê–Ø –§–£–ù–ö–¶–ò–Ø: –ü–æ–ª—É—á–∏—Ç—å –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —á–∞—Ç–∞
async def get_chat_active_users(chat_id, min_messages=10):
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —á–∞—Ç–∞ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å–æ–æ–±—â–µ–Ω–∏–π"""
    try:
        user_stats = defaultdict(lambda: {'username': None, 'full_name': None, 'count': 0})
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç—Ä–æ–∫ –ª–æ–≥–∞
        pattern = re.compile(rf".* - Chat {chat_id}\b.*User (\d+) \(([^)]+)\) \[(.+?)\]: (.*)")
        
        async with aiofiles.open(LOG_FILE, mode="r", encoding="utf-8") as f:
            async for line in f:
                match = pattern.match(line)
                if match:
                    user_id = match.group(1)
                    username = match.group(2) if match.group(2) != "NoUsername" else None
                    full_name = match.group(3) if match.group(3) != "NoName" else None
                    
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º username –∫–∞–∫ –∫–ª—é—á, –µ—Å–ª–∏ –µ—Å—Ç—å, –∏–Ω–∞—á–µ full_name
                    key = username if username else full_name
                    if key:
                        user_stats[key]['username'] = username
                        user_stats[key]['full_name'] = full_name
                        user_stats[key]['count'] += 1
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å–æ–æ–±—â–µ–Ω–∏–π
        result = []
        for key, stats in user_stats.items():
            if stats['count'] >= min_messages:
                user_data = {
                    'username': stats['username'],
                    'full_name': stats['full_name'],
                    'message_count': stats['count']
                }
                result.append(user_data)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–æ–æ–±—â–µ–Ω–∏–π
        result.sort(key=lambda x: x['message_count'], reverse=True)
        
        return result
        
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {e}")
        return []

# –í—ã–Ω–µ—Å–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ "–º–æ–π –ª–µ–∫—Å–∏–∫–æ–Ω"
async def process_my_lexicon(user_id, chat_id, message):
    # –°–æ–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ —Å–æ–æ–±—â–µ–Ω–∏—è —ç—Ç–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ —ç—Ç–æ–º —á–∞—Ç–µ
    messages = await extract_user_messages(user_id, chat_id)
    if not messages:
        await message.reply("–ù—É–ª–µ–≤–æ–π")
        return
        
    frequent_words = get_frequent_phrases_from_text(" ".join(messages), n=1, top_n=10)
    frequent_phrases = get_frequent_phrases_from_text(" ".join(messages), n=2, top_n=5)
    
    response_text = (
        "–ß–∞—Å—Ç–æ —É–ø–æ—Ç—Ä–µ–±–ª—è–µ–º—ã–µ —Å–ª–æ–≤–∞ –≤ —ç—Ç–æ–º —á–∞—Ç–µ:\n" +
        ", ".join([f"{word} ({count})" for word, count in frequent_words]) +
        "\n\n–ß–∞—Å—Ç–æ —É–ø–æ—Ç—Ä–µ–±–ª—è–µ–º—ã–µ —Ñ—Ä–∞–∑—ã:\n" +
        ", ".join([f"{phrase} ({count})" for phrase, count in frequent_phrases])
    )
    
    await message.reply(response_text)

# –í—ã–Ω–µ—Å–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ "–ª–µ–∫—Å–∏–∫–æ–Ω —á–∞—Ç"
async def process_chat_lexicon(message: types.Message) -> str:
    chat_id = message.chat.id
    frequent_words = await get_chat_frequent_words(chat_id)
    frequent_phrases = await get_chat_frequent_phrases(chat_id, n=2)
    
    response_text = (
        "–ß–∞—Å—Ç–æ —É–ø–æ—Ç—Ä–µ–±–ª—è–µ–º—ã–µ —Å–ª–æ–≤–∞ –≤ —á–∞—Ç–µ:\n" +
        "\n".join([f"{word}: {count}" for word, count in frequent_words]) +
        "\n\n–ß–∞—Å—Ç–æ —É–ø–æ—Ç—Ä–µ–±–ª—è–µ–º—ã–µ —Ñ—Ä–∞–∑—ã –≤ —á–∞—Ç–µ:\n" +
        "\n".join([f"{phrase}: {count}" for phrase, count in frequent_phrases])
    )
    return response_text

# –í—ã–Ω–µ—Å–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ "–ª–µ–∫—Å–∏–∫–æ–Ω <–∏–º—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è>"
async def process_user_lexicon(username_or_name, chat_id, message):
    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ø–æ username (–±–µ–∑ @)
    messages = await extract_messages_by_username(username_or_name, chat_id)
    
    if not messages:
        # –ï—Å–ª–∏ –ø–æ username –Ω–µ –Ω–∞—à–ª–∏, –ø—Ä–æ–±—É–µ–º –∏—Å–∫–∞—Ç—å –ø–æ –ø–æ–ª–Ω–æ–º—É –∏–º–µ–Ω–∏
        messages = await extract_messages_by_full_name(username_or_name, chat_id)
        
    if not messages:
        await message.reply(f"–°–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è '{username_or_name}' –≤ —ç—Ç–æ–º —á–∞—Ç–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
        return
        
    frequent_words = get_frequent_phrases_from_text(" ".join(messages), n=1, top_n=10)
    frequent_phrases = get_frequent_phrases_from_text(" ".join(messages), n=2, top_n=5)
    
    response_text = (
        f"–ß–∞—Å—Ç–æ —É–ø–æ—Ç—Ä–µ–±–ª—è–µ–º—ã–µ —Å–ª–æ–≤–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {username_or_name}:\n" +
        ", ".join([f"{word} ({count})" for word, count in frequent_words]) +
        "\n\n–ß–∞—Å—Ç–æ —É–ø–æ—Ç—Ä–µ–±–ª—è–µ–º—ã–µ —Ñ—Ä–∞–∑—ã:\n" +
        ", ".join([f"{phrase} ({count})" for phrase, count in frequent_phrases])
    )
    
    await message.reply(response_text)